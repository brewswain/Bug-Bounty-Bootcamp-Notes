# Scope Discovery

Now that we explained google dorking, let's dive into the actual recon. The first step is **ALWAYS** to verify the target's scope. The scope on a target's policy page specifies which subdomains, products, applications, and attack methods you're allowed to use. We don't want to overstep boundaries during the recon and hacking process.

---

## WHOIS and Reverse WHOIS

The `whois` command in our terminal can query registrant of owner information of each known domain that we're searching for:

`whois facebook.com`

Of course, this info isn't always available due to domain privacy. We can then try to conduct a reverse WHOIS search, searching a database by using an organization's name, phone number, or email address to find domains registered with it. Reverse WHOIS is super useful for more obscure or internal domains that aren't publically disclosed ot the public. An example of a public reverse WHOIS tool would be [ViewDNS.info](https://viewdns.info/reversewhois/)

---

## IP Addresses

Another way to discover top-level domains is to locate IP addresses using the `nslookup` command:

`nslookup facebook.com`

Once we get our IP Address of our known domain, we can then do areverse IP Lookup. We can also use ViewDNS for this, or even run a WHOIS command, and checking to see if they have a `NetRange` field:

```
$ whois 157.240.2.35
** NetRange: 157.240.0.0 - 157.240.255.255 **
CIDR: 157.240.0.0/16
NetName: THEFA-3
NetHandle: NET-157-240-0-0-1
Parent: NET157 (NET-157-0-0-0-0)
NetType: Direct Assignment
OriginAS:
Organization: Facebook, Inc. (THEFA-3)
RegDate: 2015-05-14
Updated: 2015-05-14
Ref: https://rdap.arin.net/registry/ip/157.240.0.0
OrgName: Facebook, Inc.
OrgId: THEFA-3
Address: 1601 Willow Rd.
City: Menlo Park
StateProv: CA
Web Hacking Reconnaissance   67
PostalCode: 94025
Country: US
RegDate: 2004-08-11
Updated: 2012-04-17
Ref: https://rdap.arin.net/registry/entity/THEFA-3
OrgAbuseHandle: OPERA82-ARIN
OrgAbuseName: Operations
OrgAbusePhone: +1-650-543-4800
OrgAbuseEmail: noc@fb.com
OrgAbuseRef: https://rdap.arin.net/registry/entity/OPERA82-ARIN
OrgTechHandle: OPERA82-ARIN
OrgTechName: Operations
OrgTechPhone: +1-650-543-4800
OrgTechEmail: noc@fb.com
OrgTechRef: https://rdap.arin.net/registry/entity/OPERA82-ARIN
```

Another way of finding IP addresses in scope is by looking at autonomous
systems, which are routable networks within the public internet. Autonomous
system numbers (ASNs) identify the owners of these networks. By checking if
two IP addresses share an ASN, you can determine whether the IPs belong to
the same owner.
To figure out if a company owns a dedicated IP range, run several IP-toASN translations to see if the IP addresses map to a single ASN. If many
addresses within a range belong to the same ASN, the organization might
have a dedicated IP range. From the following output, we can deduce that
any IP within the 157.240.2.21 to 157.240.2.34 range probably belongs to
Facebook:

```
whois -h whois.cymru.com 157.240.2.20
AS | IP | AS Name
32934 | 157.240.2.20 | FACEBOOK, US
whois -h whois.cymru.com 157.240.2.27
AS | IP | AS Name
32934 | 157.240.2.27 | FACEBOOK, US
whois -h whois.cymru.com 157.240.2.35
AS | IP | AS Name
32934 | 157.240.2.35 | FACEBOOK, US
```

The -h flag that we're using sets the WHOIS server to retrieve information from. whois.cymru is a database that translates IPs to ASNs. Therefore, if the company has a dedicated IP range and doesn't mark those IP addresses as out of scope, we can attack every single IP in that range.

---

## Certificate Parsing

We can take advantage of SSL certificates used to encrypt web traffic. An SSL certificate has a `Subject Alternative Name` which lets cert owners specify additional hostnames that use the same cert, so we can find those hostnames by parsing this field. Databases to help with this include [crt.sh](crt.sh), [Censys](https://censys.io/), and [Cert Spotter](https://sslmate.com/certspotter/)

For example, by running a certificate search using crt.sh for facebook.com,
we can find Facebook’s SSL certificate. You’ll see that that many other domain
names belonging to Facebook are listed:

```
X509v3 Subject Alternative Name:
 DNS:*.facebook.com
 DNS:*.facebook.net
 DNS:*.fbcdn.net
 DNS:*.fbsbx.com
 DNS:*.messenger.com
 DNS:facebook.com
 DNS:messenger.com
 DNS:*.m.facebook.com
 DNS:*.xx.fbcdn.net
 DNS:*.xy.fbcdn.net
 DNS:*.xz.fbcdn.net
```

The crt.sh website also has a useful utility that lets you retrieve the
information in JSON format, rather than HTML, for easier parsing. Just
add the URL parameter output=json to the request URL:

[https://crt.sh/?q=facebook.com&output=json](https://crt.sh/?q=facebook.com&output=json)

---

## Subdomain Enumeration

Okay, so we found as many domains as possible. Our next step is to locate as many subdomains on those domains as we can. Each subdomain is a potential attack vector, so really, the more the better. For something like this...we can use some automated tools, such as:

-  Sublist3r
-  SubBrute
-  Amass
-  Gobuster

The above tools use a variety of wordlists and strategies, so it's useful to use multiple tools. Obviously, this means that using multiple tools would give us the best confluence, so we'll discuss automation/scripting strategies for that in the future.

I mentioned wordlists earlier, and we can find some good wordlists made by other hackers online. A pretty extensive one is [Daniel Miessler’s SecLists](https://github.com/danielmiessler/SecLists/).

We can also use a wordlist generator like [Commonspeak2](https://github.com/
assetnote/commonspeak2/). Once again, we can collate/combine several wordlists by using a simple command to remove duplicates from a set of two wordlists:  `sort -u wordlist1.txt wordlist2.txt`

Following this vein, if we were to use Gobuster in its DNS mode, we can use the `-d` flag to specify the domain we want to brute-force, and `-w` to specify the wordlist you want to use:

`gobuster dns -d target_domain -w wordlist`

Once we found a good number of subdomains, we can discover more by identifying patterns. Lets say we find 2 subdomains of `example.com` : `1.example.com` and `3.example.com`.
It's pretty reasonable that `2.example.com` exists. Of course, we can also check for a `4.example.com`, etc

DOing this by hand...well, we ain't hackers for nothing, so we have a good tool for automating this process: [Altdns](https://github.com/infosec-au/altdns/)

Altdns discovers subdomains with names that are permutations of other subdomain names. Also, of course, if we know the tech used in the target's stack, we can look for tech specific subdomains. And don't forget that subdomains can have other subdomains! for example, `dev.example.com` might have the subdomain of `1.dev.example.com`. We can search for these by running enumeration tools recursively by adding the results of our first run to our new Known Domains list, and run the tool again.

---

## Service Enumeration

Our next step is to enumerate the services that are hosted on the machines we found. These services tend to run on default ports, so we can find them by port-scanning the machine using either _active_ or _passive_ scanning.

### Active Scanning

With active scanning, we directly engage with the server. Active scanning tools send requests to connect to the target's ports to look for any open ones. Recommended tools include [Nmap](https://nmap.org/) and [Masscan](https://github.com/robertdavidgraham/masscan):

```
nmap scanme.nmap.org

Nmap scan report for scanme.nmap.org (45.33.32.156)
Host is up (0.086s latency).
Other addresses for scanme.nmap.org (not scanned): 2600:3c01::f03c:91ff:fe18:bb2f
Not shown: 993 closed ports
PORT STATE SERVICE
22/tcp open ssh
25/tcp filtered smtp
80/tcp open http
135/tcp filtered msrpc
445/tcp filtered microsoft-ds
9929/tcp open nping-echo
31337/tcp open Elite
Nmap done: 1 IP address (1 host up) scanned in 230.83 seconds
```

### Passive Scanning

With Passive scanning, we use third-party resources to learn about a machine's ports without interacting with it. Passive scanning tends to be stealthier and helps about detection. One possible service to use for this is [Shodan](https://www.shodan.io/) (ty based system shock).

If we were to run a Shodan search on `scanme.nmap.org`'s IP address (45.33.32.156), we'll get a bunch of information that yields different data than in our port scan, as well as some additional info about the server. Some alternatives to shodan include [Censys](https://censys.io/) and [Project Sonar](https://www.rapid7.com/research/project-sonar/). As always, confluence is pretty good, so combining results is always a good idea. We might even find our target's IP addresses, certs, and software versions.

---

## Directory Brute-forcing

Next, we can perform brute-forcing of the web servers' directories. Finding directories on servers is useful since we might discover hidden admen panels, config files, password files, outdated functionalities, database copies, source code files, etc. Sometimes we might even be able to directly take over a server this way!

Also, even without any exploits bering immediately found, the directory information can often inform us about the structure and technology of an app. For example, a pathname including _phpmyadmin_ tends to mean that the app is built with PHP.

We can use [Dirsearch](https://github.com/maurosoria/dirsearch) or [Gobuster](https://github.com/OJ/gobuster) for directory brute-forcing. These tools use wordlists to construct URLs, which they then request from a web server. Based on if we get 200s from our server or not, we'll know what files/directories exist. We can then browse to that page and see what the app is hosting there! Receiving a 403 lets us know that the target exists, but is protected. Make note of these occurrences and check carefully to see if we can bypass protections to access the content.

Let's look at what a Dirsearch command returns:

```
./dirsearch.py -u scanme.nmap.org -e php
Extensions: php | HTTP method: get | Threads: 10 | Wordlist size: 6023
Error Log: /tools/dirsearch/logs/errors.log
Target: scanme.nmap.org
[12:31:11] Starting:
[12:31:13] 403 - 290B - /.htusers
[12:31:15] 301 - 316B - /.svn -> http://scanme.nmap.org/.svn/
[12:31:15] 403 - 287B - /.svn/
[12:31:15] 403 - 298B - /.svn/all-wcprops
[12:31:15] 403 - 294B - /.svn/entries
[12:31:15] 403 - 297B - /.svn/prop-base/
[12:31:15] 403 - 296B - /.svn/pristine/
[12:31:15] 403 - 291B - /.svn/tmp/
[12:31:15] 403 - 315B - /.svn/text-base/index.php.svn-base
[12:31:15] 403 - 293B - /.svn/props/
[12:31:15] 403 - 297B - /.svn/text-base/
[12:31:40] 301 - 318B - /images -> http://scanme.nmap.org/images/
[12:31:40] 200 - 7KB - /index
[12:31:40] 200 - 7KB - /index.html
[12:31:53] 403 - 295B - /server-status
[12:31:53] 403 - 296B - /server-status/
[12:31:54] 301 - 318B - /shared -> http://scanme.nmap.org/shared/
Task Completed
```

On the other hand, Gobuster's Dir mode is used to find additional content on a specific domain or subdomain. This includes hidden directories and files. In this made, we use the -u flag to specifiy the domain/subdomain you want to brute-force, and `-w` to specify whichy wordlist we want to use:

`gobuster dir -u TARGET_URL -w WORDLIST`

Once we get all the pages through brute-forcing, manually visiting them can be pretty time-consuming. Thankfully, [EyeWitness](https://github.com/FortyNorthSecurity/EyeWitness/)
or [Snapper](https://github.com/dxa4481/Snapper/) to automatically verify that a page is hosted on each location. Eyewitness accepts a list of URLs, and takes a screenshot of each page. This is a pretty good way to find pages like admin panels, outdated pages, etc.

---

## Spidering the Site

An alternate way of discovering directories and paths is through web crawling. A web spider tool starts with a page to visit. After that, it identifies all URLs embedded on the page and visits them recursively. This allows it to uncover a bunch of hidden endpoints!

[OWASP Zed Attack Proxy(aka ZAP)](https://www.zaproxy.org) has a built-in, Open-source web spider that we can use. Burp Suite also has an equivalent tool called the crawler, but let's assume that we'll be using ZAP.

Using ZAP's as easy as opening it, choosing Tools > Spider. From there we input the starting URL, and hit start scan. Et Voila!

---

## Third-Party Hosting

Ok, so what if our target's using something like S3 buckets? Well, these buckets can contain stuff like logs, credentials, user information, etc that might be super useful to us. We can use Google Dorking to find some buckets, since most buckets use the URL format of `BUCKET.s3.amazonaws.com` or `s3.amazonaws.com/BUCKET`, meaning that these search terms are fairly useful:

```
site:s3.amazonaws.com COMPANY_NAME
site:amazonaws.com COMPANY_NAME
```

Now, if the company uses custom URLs for its buckets, all hope is not lost, since we can simply loosen up our query instead:

```
amazonaws s3 COMPANY_NAME
amazonaws bucket COMPANY_NAME
amazonaws COMPANY_NAME
s3 COMPANY_NAME
```

We can also look for buckets via said company's public Github repositories for S3 URLs. Realistically, these URLs _should_ be hidden via .env files, but github recon is very powerful.

We can use [GrayHatWarfare](https://buckets.grayhatwarfare.com) to find publicly exposed S3 buckets in a search engine format.

Finally, we can also try ye olde time-honoured method of brute-forcing using keywords. [Lazys3](http0s://github.com/nahamsec/lazys3) is one such tool that'll help us do this. Using a wordlist, it'll guess buckets that are permutations of common names. Another tool is [Bucket Stream](https://github.com/eth0izzle/bucket-stream), which parses certs belonging to an organization and finds S3 buckets based on permutations of the domain names that we find on said certs. It also checks whether the bucket's accessible!

Now comes the cheeky part. We can install the awscli (pip, node, whatever), then
[configure it to work with AWS](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html). Assuming that they don't properly configure pre-signed URLs or the like, we should be able to access these buckets using the CLI.

For instance, to list the bucket's content:

```
aws s3 ls s3://BUCKET_NAME/
```

If we get anything good, we can try to copy these files to our local machine:

```
aws s3 cp s3://BUCKET_NAME/FILE_NAME/path/to/local/directory
```

Any useful info we get via the bucket can be saved for future exploitation! Stuff like API keys, personal information should be reported right away, considering that exposed buckets are often considered a vulnerability by themselves.

We can also try to interface with the buckets via PUT and DELETE commands to see if we can tamper with the app's operations or even corrupt company data. For a non-destructive approach, let's copy a local file to the target's bucket:

```
aws s3 cp TEST_FILE s3://BUCKET_NAME/
```

and then let's delete it:

```
aws s3 rm s3://BUCKET_NAME/TEST_FILE
```

Don't forget to document this if it works, as it's solid proof that we have write access to these buckets without touching the target company's files.

---

## Github Recon

It's super easy to accidentally commit data to github repos that can be potentially sensitive or dangerous. Therefore, it's pretty useful to find github usernames relevant to our target. This should be findable by searching for the target's organization/product names via github, or even searching known employees' github accounts.

Once we found the usernames we wish to audit, we can visit their pages and find repos related to the projects we're testing for and record them, along with the usernames of the org's top contributors, which acts like a domino that can lead us into more relevant repos.

Once that's done with, if we find some public repositories, we can dive into the code and pay attention to the Issues and Commits sections in particular. It's...really common for screwups or problematic code to be highlighted there, as well asw show potential attack vectors due to security patches that haven't gotten merged yet.

Keep an eye out for that stuff! Super recent code changes are often the most suspect since we haven't seen if they can stand the test of time yet. Also, this lets us see what protection mechanisms are implemented, and maybe even search directly for common code snippet vulns. If we find an interesting file, we can check the Blame and History sections for more details and timeline.

Also, anything obvious such as hardcoded secrets should be searched for--terms like `password`, `key`, `secret` are all fair game.

Let's say we find these leaked creds, we can use [KeyHacks](https://github.com/streaak/keyhacks) to check if the creds are valid and learn how to use em.

We can also search for sensitive functionalities in the project, such as stuff like auth, password resets, state-changing actions, private info reads.

Also, pay attention to code that deals with user inputs, such as HTTP request stuff (params, heads, fetch requests, etc), as well as database entries, file reads, uploads since they all provide potential entry points for exploits. Also, any config files found help paint the picture of our target's infra. We can also look for old endpoints and Bucket URLs for testing, which should be recorded.

Also. And this one is huge. Even though we have features like dependabot etc, chances are that any codebase will have some outdated dependencies. This is a huge source of bugs, and should be checked along with what imports are being used. Any outdated dependencies that we find should be recorded for research later to see if we can find any publicly disclosed vulns. It may be efficient to simply copy the packet.json and run an npm audit or something, I'll figure that out later.

Now, as we can imagine, this is a pretty intesnive process. Thankfully, [Gitrob](https://github.com/michenriksen/gitrob) and [TruffleHog](https://github.com//trufflesecurity/truffleHog) can help us automate github recon, with Gitrob locating potentially sensitive files pushed ot public repos, and TruffleHog specializing in finding secrets in repos using regex.
